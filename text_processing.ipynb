{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download(\"wordnet\")\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [{'text': ['üôè', 'üá∫üá∏', ' Peace to all politics'...\n",
       "1                                                    []\n",
       "2                                                    []\n",
       "3                                                    []\n",
       "4                                                    []\n",
       "5                                                    []\n",
       "6                                                    []\n",
       "7     [{'text': ['It was hilarious. He represents th...\n",
       "8     [{'text': ['Yes I do, but at the expense of ma...\n",
       "9     [{'text': ['Please keep seeking God‚Äôs will for...\n",
       "10    [{'text': ['Good this destruction is costing m...\n",
       "11    [{'text': [\"How can the media get away with su...\n",
       "12    [{'text': ['Thankful for you getting the news ...\n",
       "13    [{'text': ['Is this the same Fox News that you...\n",
       "14    [{'text': [\"Please sir bring in the national g...\n",
       "15    [{'text': ['PRESIDENT YOUR AWESOME.No one else...\n",
       "16                                                   []\n",
       "17                                                   []\n",
       "18                                                   []\n",
       "19                                                   []\n",
       "20    [{'text': ['Thank you for spending your late y...\n",
       "21                                                   []\n",
       "22                                                   []\n",
       "23                                                   []\n",
       "24    [{'text': ['How well has it helped you?', \" Sh...\n",
       "Name: comments, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('profile_data.csv')\n",
    "df['comments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[[{'text': ['üôè',  'üá∫üá∏',  ' Peace to all politi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>[[{'text': ['It was hilarious. He represents t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>[[{'text': ['Yes I do,  but at the expense of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>[[{'text': ['Please keep seeking God‚Äôs will fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>[[{'text': ['Good this destruction is costing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>[[{'text': [\"How can the media get away with s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>[[{'text': ['Thankful for you getting the news...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>[[{'text': ['Is this the same Fox News that yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>[[{'text': [\"Please sir bring in the national ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>[[{'text': ['PRESIDENT YOUR AWESOME.No one els...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index                                           comments\n",
       "0       0  [[{'text': ['üôè',  'üá∫üá∏',  ' Peace to all politi...\n",
       "1       1                                               [[]]\n",
       "2       2                                               [[]]\n",
       "3       3                                               [[]]\n",
       "4       4                                               [[]]\n",
       "5       5                                               [[]]\n",
       "6       6                                               [[]]\n",
       "7       7  [[{'text': ['It was hilarious. He represents t...\n",
       "8       8  [[{'text': ['Yes I do,  but at the expense of ...\n",
       "9       9  [[{'text': ['Please keep seeking God‚Äôs will fo...\n",
       "10     10  [[{'text': ['Good this destruction is costing ...\n",
       "11     11  [[{'text': [\"How can the media get away with s...\n",
       "12     12  [[{'text': ['Thankful for you getting the news...\n",
       "13     13  [[{'text': ['Is this the same Fox News that yo...\n",
       "14     14  [[{'text': [\"Please sir bring in the national ...\n",
       "15     15  [[{'text': ['PRESIDENT YOUR AWESOME.No one els...\n",
       "16     16                                               [[]]\n",
       "17     17                                               [[]]\n",
       "18     18                                               [[]]\n",
       "19     19                                               [[]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments=df['comments'].str.split(',').reset_index()\n",
    "comments.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[[{'text': ['üôè',  'üá∫üá∏',  ' Peace to all politi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>[[{'text': ['It was hilarious. He represents t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>[[{'text': ['Yes I do,  but at the expense of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>[[{'text': ['Please keep seeking God‚Äôs will fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>[[{'text': ['Good this destruction is costing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>[[{'text': [\"How can the media get away with s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>[[{'text': ['Thankful for you getting the news...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>[[{'text': ['Is this the same Fox News that yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>[[{'text': [\"Please sir bring in the national ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>[[{'text': ['PRESIDENT YOUR AWESOME.No one els...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>[[{'text': ['Thank you for spending your late ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>[[{'text': ['How well has it helped you?',  \" ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index                                           comments\n",
       "0       0  [[{'text': ['üôè',  'üá∫üá∏',  ' Peace to all politi...\n",
       "1       1                                               [[]]\n",
       "2       2                                               [[]]\n",
       "3       3                                               [[]]\n",
       "4       4                                               [[]]\n",
       "5       5                                               [[]]\n",
       "6       6                                               [[]]\n",
       "7       7  [[{'text': ['It was hilarious. He represents t...\n",
       "8       8  [[{'text': ['Yes I do,  but at the expense of ...\n",
       "9       9  [[{'text': ['Please keep seeking God‚Äôs will fo...\n",
       "10     10  [[{'text': ['Good this destruction is costing ...\n",
       "11     11  [[{'text': [\"How can the media get away with s...\n",
       "12     12  [[{'text': ['Thankful for you getting the news...\n",
       "13     13  [[{'text': ['Is this the same Fox News that yo...\n",
       "14     14  [[{'text': [\"Please sir bring in the national ...\n",
       "15     15  [[{'text': ['PRESIDENT YOUR AWESOME.No one els...\n",
       "16     16                                               [[]]\n",
       "17     17                                               [[]]\n",
       "18     18                                               [[]]\n",
       "19     19                                               [[]]\n",
       "20     20  [[{'text': ['Thank you for spending your late ...\n",
       "21     21                                               [[]]\n",
       "22     22                                               [[]]\n",
       "23     23                                               [[]]\n",
       "24     24  [[{'text': ['How well has it helped you?',  \" ..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "list2=comments['comments'][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"[{'text': ['It was hilarious. He represents the Democrat party perfectly! '\", \" 'ü•¥']\", \" 'profile_name': 'Alicia Nielsen'\", \" 'profile_url': '/alicia.nielsen.94'}\", \" {'text': ['It‚Äôs all corrupt. They are literally creating their own narrative. Everything scripted. Everything ‚Äúperfect‚Äù. Those of us who are paying attention see it. God will win in the end.']\", \" 'profile_name': 'Blake Hickey'\", \" 'profile_url': '/blake.hickey.14'}\", \" {'text': ['That press conference was a sham.  It was obvious he knew the questions ahead of time.  Biden is very low energy and frail.  Trump2020']\", \" 'profile_name': 'Dar Garcia'\", \" 'profile_url': '/profile.php'}\", \" {'text': ['He will be given questions at the three debates']\", \" 'profile_name': 'Gail M. Fuller'\", \" 'profile_url': '/gail.m.fuller.7'}\", \" {'text': ['This was the only way he could answer the questions is to get them ahead of time!']\", \" 'profile_name': 'Lois Petelinkar'\", \" 'profile_url': '/lois.petelinkar'}\", ' {\\'text\\': [\"You\\'ve got my vote!! Thank you for being a great leader\"]', \" 'profile_name': 'Jenn Fah Fah'\", \" 'profile_url': '/jenfahfah'}\", \" {'text': ['You would have performed dismally.']\", \" 'profile_name': 'Philip Nyamai'\", \" 'profile_url': '/philip.nyamai.3'}\", \" {'text': ['Well that‚Äôs because they didn‚Äôt want him to tell the world he was going to beat ‚ÄúDonald Reagan again‚Äù '\", \" 'ü§£'\", \" 'ü§£'\", \" 'ü§£']\", \" 'profile_name': 'Erin Rachelle'\", \" 'profile_url': '/profile.php'}\", \" {'text': ['We have gotten an increase in our social security each year President Trump has been in office. During the Obama Biden Administration we did not get an increase in our social security each year. This information needs to become widespread.']\", \" 'profile_name': 'Claudia Bourgeois'\", \" 'profile_url': '/profile.php'}\", \" {'text': ['He said he was given a list of whom to take questions from and in what order with their pre-given questions to ask.'\", \" ' Baby Boy Biden mentioned twice he was going to be in trouble!'\", \" ' This man is being exploited and it is incredibly sad! Where the hell is his family who should step in and rescue him to spend the time he has left with dignity!!!!!']\", \" 'profile_name': 'Marti McConnell'\", \" 'profile_url': '/marti.kurthminer'}\", ' {\\'text\\': [\"It\\'s sort of sad really.   I believe the establishment has forced all of this on him and he truly is no longer capable of doing it.   They\\'re in a tough spot for sure.\"]', \" 'profile_name': 'Harry Windsor'\", \" 'profile_url': '/HarryLeeWindsor'}\", \" {'text': ['It‚Äôs sad his family is letting him run for office. Very obvious he was reading his answers!']\", \" 'profile_name': 'Susan Thompson'\", \" 'profile_url': '/profile.php'}\", \" {'text': ['It was terrible. Calling on reporters like they were 4th grade students asking a guest speaker planned 1st grade questions. Unbelievable what extremes they will resort to - actually is unconscionable -this is very sad for Joe Biden and his family.']\", \" 'profile_name': 'Shari Ann'\", \" 'profile_url': '/shacapp'}\", \" {'text': ['If he has been tested for cognitive decline\", ' then what were the results?  And if he gets tested regularly', \" then why?   Great debate question ']\", \" 'profile_name': 'Matt Reynolds'\", \" 'profile_url': '/financialmattrs'}\", \" {'text': ['Actually as I watched he appeared to look down on his paper on the podium while he was answering the questions. He also read from his paper the name of the person who was next. Made me wonder if he knew what the reporters would ask ahead of the press conference.']\", \" 'profile_name': 'Beverly Stevens'\", \" 'profile_url': '/beverly.stevens.96592'}\", \" {'text': ['Well he can not answer them on his own. His wife probably writes the answers for him! This should make everyone feel safe '\", \" 'üòÇ']\", \" 'profile_name': 'Cheryl Scully-Cappello'\", \" 'profile_url': '/cheryl.scullycappello'}\", \" {'text': ['He can‚Äôt remember his name let alone answers to questions of course they have to give him the answers he‚Äôs their puppet.']\", \" 'profile_name': 'Sarah Southern'\", \" 'profile_url': '/sarah.southern.58'}\", \" {'text': ['You know that he WILL NOT be showing for the debates. I am going to guess here that he will have contracted covid and Will not be able to hold them due to him being temporarily being sick AND of course forcing the necessity of a mandated vaccine produced by Glaxo where Gates sold his covid patent.']\", \" 'profile_name': 'Elaine Landry'\", \" 'profile_url': '/elaine.landry.378'}\", \" {'text': ['I feel sorry for the guy (Biden)His brain is deteriorating and no one is stopping him!!He definitely has dementia!!']\", \" 'profile_name': 'Miguel Nave'\", \" 'profile_url': '/miguel.nave.50'}\", \" {'text': ['Absolutely\", \" I watched it too and Biden knew exactly what questions were coming. '\", \" ' I cannot believe people will vote for this man. He‚Äôs hopeless. ']\", \" 'profile_name': 'Gary Williams'\", \" 'profile_url': '/profile.php'}]\"]\n"
     ]
    }
   ],
   "source": [
    "print(list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"[{'text': ['It was hilarious. He represents the Democrat party perfectly! '\", \" 'profile_url': '/blake.hickey.14'}\", \" 'profile_url': '/gail.m.fuller.7'}\", \" 'profile_url': '/jenfahfah'}\", \" 'ü§£'\", \" 'profile_url': '/profile.php'}\", ' {\\'text\\': [\"It\\'s sort of sad really.   I believe the establishment has forced all of this on him and he truly is no longer capable of doing it.   They\\'re in a tough spot for sure.\"]', \" {'text': ['It was terrible. Calling on reporters like they were 4th grade students asking a guest speaker planned 1st grade questions. Unbelievable what extremes they will resort to - actually is unconscionable -this is very sad for Joe Biden and his family.']\", \" 'profile_name': 'Matt Reynolds'\", \" 'üòÇ']\"]\n"
     ]
    }
   ],
   "source": [
    "list3=list2[0:-1:6]\n",
    "list4=list3[0:10]\n",
    "print(list4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'text': ['It was hilarious. He represents th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'profile_url': '/blake.hickey.14'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'profile_url': '/gail.m.fuller.7'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'profile_url': '/jenfahfah'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'ü§£'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>'profile_url': '/profile.php'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{'text': [\"It's sort of sad really.   I belie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{'text': ['It was terrible. Calling on report...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>'profile_name': 'Matt Reynolds'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>'üòÇ']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment\n",
       "0  [{'text': ['It was hilarious. He represents th...\n",
       "1                 'profile_url': '/blake.hickey.14'}\n",
       "2                 'profile_url': '/gail.m.fuller.7'}\n",
       "3                       'profile_url': '/jenfahfah'}\n",
       "4                                                'ü§£'\n",
       "5                     'profile_url': '/profile.php'}\n",
       "6   {'text': [\"It's sort of sad really.   I belie...\n",
       "7   {'text': ['It was terrible. Calling on report...\n",
       "8                    'profile_name': 'Matt Reynolds'\n",
       "9                                               'üòÇ']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "df2=DataFrame(list4,columns=[\"comment\"])\n",
    "df2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['{', \"'text\", \"'\", ':', '[', \"'It\", 'was', 'terrible', '.', 'Calling', 'on', 'reporters', 'like', 'they', 'were', '4th', 'grade', 'students', 'asking', 'a', 'guest', 'speaker', 'planned', '1st', 'grade', 'questions', '.', 'Unbelievable', 'what', 'extremes', 'they', 'will', 'resort', 'to', '-', 'actually', 'is', 'unconscionable', '-this', 'is', 'very', 'sad', 'for', 'Joe', 'Biden', 'and', 'his', 'family', '.', \"'\", ']']\n"
     ]
    }
   ],
   "source": [
    "word_tokens=nltk.word_tokenize(df2[\"comment\"][7])\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokens = nltk.word_tokenize(df2[\"comment\"][7])\n",
    "new_words= [word for word in word_tokens if word.isalnum()]\n",
    "\n",
    "print(new_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of Speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('was', 'VBD'),\n",
       " ('terrible', 'JJ'),\n",
       " ('Calling', 'VBG'),\n",
       " ('on', 'IN'),\n",
       " ('reporters', 'NNS'),\n",
       " ('like', 'IN'),\n",
       " ('they', 'PRP'),\n",
       " ('were', 'VBD'),\n",
       " ('4th', 'CD'),\n",
       " ('grade', 'JJ'),\n",
       " ('students', 'NNS'),\n",
       " ('asking', 'VBG'),\n",
       " ('a', 'DT'),\n",
       " ('guest', 'NN'),\n",
       " ('speaker', 'NN'),\n",
       " ('planned', 'VBD'),\n",
       " ('1st', 'CD'),\n",
       " ('grade', 'NN'),\n",
       " ('questions', 'NNS'),\n",
       " ('Unbelievable', 'VBP'),\n",
       " ('what', 'WP'),\n",
       " ('extremes', 'VBZ'),\n",
       " ('they', 'PRP'),\n",
       " ('will', 'MD'),\n",
       " ('resort', 'VB'),\n",
       " ('to', 'TO'),\n",
       " ('actually', 'RB'),\n",
       " ('is', 'VBZ'),\n",
       " ('unconscionable', 'JJ'),\n",
       " ('is', 'VBZ'),\n",
       " ('very', 'RB'),\n",
       " ('sad', 'JJ'),\n",
       " ('for', 'IN'),\n",
       " ('Joe', 'NNP'),\n",
       " ('Biden', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('his', 'PRP$'),\n",
       " ('family', 'NN')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(new_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stop word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['terrible', 'Calling', 'reporters', 'like', '4th', 'grade', 'students', 'asking', 'guest', 'speaker', 'planned', '1st', 'grade', 'questions', 'Unbelievable', 'extremes', 'resort', 'actually', 'unconscionable', 'sad', 'Joe', 'Biden', 'family']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english')) \n",
    "filtered_sentence = [w for w in new_words if not w in stop_words]\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "she --> she\n",
      "didn --> didn\n",
      "yourself --> yourself\n",
      "have --> have\n",
      "y --> y\n",
      "itself --> itself\n",
      "out --> out\n",
      "in --> in\n",
      "not --> not\n",
      "wouldn --> wouldn\n",
      "shan --> shan\n",
      "her --> her\n",
      "about --> about\n",
      "haven't --> haven't\n",
      "again --> again\n",
      "d --> d\n",
      "so --> so\n",
      "against --> against\n",
      "we --> we\n",
      "does --> doe\n",
      "more --> more\n",
      "had --> had\n",
      "i --> i\n",
      "because --> becaus\n",
      "be --> be\n",
      "why --> whi\n",
      "you've --> you'v\n",
      "herself --> herself\n",
      "very --> veri\n",
      "don --> don\n",
      "for --> for\n",
      "ain --> ain\n",
      "who --> who\n",
      "few --> few\n",
      "am --> am\n",
      "hadn --> hadn\n",
      "o --> o\n",
      "it's --> it\n",
      "these --> these\n",
      "s --> s\n",
      "don't --> don't\n",
      "needn --> needn\n",
      "wouldn't --> wouldn't\n",
      "where --> where\n",
      "couldn --> couldn\n",
      "all --> all\n",
      "from --> from\n",
      "do --> do\n",
      "further --> further\n",
      "she's --> she\n",
      "on --> on\n",
      "no --> no\n",
      "is --> is\n",
      "he --> he\n",
      "me --> me\n",
      "hers --> her\n",
      "as --> as\n",
      "to --> to\n",
      "your --> your\n",
      "ours --> our\n",
      "myself --> myself\n",
      "yourselves --> yourselv\n",
      "aren't --> aren't\n",
      "mustn't --> mustn't\n",
      "being --> be\n",
      "weren --> weren\n",
      "while --> while\n",
      "ma --> ma\n",
      "into --> into\n",
      "t --> t\n",
      "than --> than\n",
      "the --> the\n",
      "aren --> aren\n",
      "before --> befor\n",
      "under --> under\n",
      "was --> was\n",
      "yours --> your\n",
      "hasn --> hasn\n",
      "of --> of\n",
      "re --> re\n",
      "by --> by\n",
      "a --> a\n",
      "himself --> himself\n",
      "didn't --> didn't\n",
      "won --> won\n",
      "are --> are\n",
      "nor --> nor\n",
      "isn't --> isn't\n",
      "should've --> should'v\n",
      "whom --> whom\n",
      "through --> through\n",
      "wasn't --> wasn't\n",
      "doesn't --> doesn't\n",
      "his --> his\n",
      "over --> over\n",
      "here --> here\n",
      "ll --> ll\n",
      "now --> now\n",
      "they --> they\n",
      "which --> which\n",
      "at --> at\n",
      "needn't --> needn't\n",
      "shouldn --> shouldn\n",
      "or --> or\n",
      "our --> our\n",
      "my --> my\n",
      "some --> some\n",
      "isn --> isn\n",
      "mustn --> mustn\n",
      "theirs --> their\n",
      "when --> when\n",
      "there --> there\n",
      "each --> each\n",
      "how --> how\n",
      "but --> but\n",
      "if --> if\n",
      "both --> both\n",
      "should --> should\n",
      "them --> them\n",
      "doesn --> doesn\n",
      "too --> too\n",
      "own --> own\n",
      "mightn --> mightn\n",
      "mightn't --> mightn't\n",
      "can --> can\n",
      "that'll --> that'll\n",
      "you --> you\n",
      "weren't --> weren't\n",
      "such --> such\n",
      "ourselves --> ourselv\n",
      "haven --> haven\n",
      "up --> up\n",
      "m --> m\n",
      "down --> down\n",
      "then --> then\n",
      "an --> an\n",
      "has --> has\n",
      "just --> just\n",
      "only --> onli\n",
      "between --> between\n",
      "wasn --> wasn\n",
      "any --> ani\n",
      "same --> same\n",
      "those --> those\n",
      "hadn't --> hadn't\n",
      "themselves --> themselv\n",
      "were --> were\n",
      "been --> been\n",
      "doing --> do\n",
      "did --> did\n",
      "shan't --> shan't\n",
      "you'll --> you'll\n",
      "during --> dure\n",
      "other --> other\n",
      "shouldn't --> shouldn't\n",
      "after --> after\n",
      "you'd --> you'd\n",
      "hasn't --> hasn't\n",
      "that --> that\n",
      "won't --> won't\n",
      "once --> onc\n",
      "ve --> ve\n",
      "above --> abov\n",
      "with --> with\n",
      "couldn't --> couldn't\n",
      "will --> will\n",
      "their --> their\n",
      "off --> off\n",
      "it --> it\n",
      "you're --> you'r\n",
      "what --> what\n",
      "and --> and\n",
      "until --> until\n",
      "below --> below\n",
      "having --> have\n",
      "this --> this\n",
      "its --> it\n",
      "him --> him\n",
      "most --> most\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "s_stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "for word in stop_words:\n",
    "    print(word+' --> '+s_stemmer.stem(word))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "she --> she\n",
      "didn --> didn\n",
      "yourself --> yourself\n",
      "have --> have\n",
      "y --> y\n",
      "itself --> itself\n",
      "out --> out\n",
      "in --> in\n",
      "not --> not\n",
      "wouldn --> wouldn\n",
      "shan --> shan\n",
      "her --> her\n",
      "about --> about\n",
      "haven't --> haven't\n",
      "again --> again\n",
      "d --> d\n",
      "so --> so\n",
      "against --> against\n",
      "we --> we\n",
      "does --> doe\n",
      "more --> more\n",
      "had --> had\n",
      "i --> i\n",
      "because --> because\n",
      "be --> be\n",
      "why --> why\n",
      "you've --> you've\n",
      "herself --> herself\n",
      "very --> very\n",
      "don --> don\n",
      "for --> for\n",
      "ain --> ain\n",
      "who --> who\n",
      "few --> few\n",
      "am --> am\n",
      "hadn --> hadn\n",
      "o --> o\n",
      "it's --> it's\n",
      "these --> these\n",
      "s --> s\n",
      "don't --> don't\n",
      "needn --> needn\n",
      "wouldn't --> wouldn't\n",
      "where --> where\n",
      "couldn --> couldn\n",
      "all --> all\n",
      "from --> from\n",
      "do --> do\n",
      "further --> further\n",
      "she's --> she's\n",
      "on --> on\n",
      "no --> no\n",
      "is --> is\n",
      "he --> he\n",
      "me --> me\n",
      "hers --> hers\n",
      "as --> a\n",
      "to --> to\n",
      "your --> your\n",
      "ours --> ours\n",
      "myself --> myself\n",
      "yourselves --> yourselves\n",
      "aren't --> aren't\n",
      "mustn't --> mustn't\n",
      "being --> being\n",
      "weren --> weren\n",
      "while --> while\n",
      "ma --> ma\n",
      "into --> into\n",
      "t --> t\n",
      "than --> than\n",
      "the --> the\n",
      "aren --> aren\n",
      "before --> before\n",
      "under --> under\n",
      "was --> wa\n",
      "yours --> yours\n",
      "hasn --> hasn\n",
      "of --> of\n",
      "re --> re\n",
      "by --> by\n",
      "a --> a\n",
      "himself --> himself\n",
      "didn't --> didn't\n",
      "won --> won\n",
      "are --> are\n",
      "nor --> nor\n",
      "isn't --> isn't\n",
      "should've --> should've\n",
      "whom --> whom\n",
      "through --> through\n",
      "wasn't --> wasn't\n",
      "doesn't --> doesn't\n",
      "his --> his\n",
      "over --> over\n",
      "here --> here\n",
      "ll --> ll\n",
      "now --> now\n",
      "they --> they\n",
      "which --> which\n",
      "at --> at\n",
      "needn't --> needn't\n",
      "shouldn --> shouldn\n",
      "or --> or\n",
      "our --> our\n",
      "my --> my\n",
      "some --> some\n",
      "isn --> isn\n",
      "mustn --> mustn\n",
      "theirs --> theirs\n",
      "when --> when\n",
      "there --> there\n",
      "each --> each\n",
      "how --> how\n",
      "but --> but\n",
      "if --> if\n",
      "both --> both\n",
      "should --> should\n",
      "them --> them\n",
      "doesn --> doesn\n",
      "too --> too\n",
      "own --> own\n",
      "mightn --> mightn\n",
      "mightn't --> mightn't\n",
      "can --> can\n",
      "that'll --> that'll\n",
      "you --> you\n",
      "weren't --> weren't\n",
      "such --> such\n",
      "ourselves --> ourselves\n",
      "haven --> haven\n",
      "up --> up\n",
      "m --> m\n",
      "down --> down\n",
      "then --> then\n",
      "an --> an\n",
      "has --> ha\n",
      "just --> just\n",
      "only --> only\n",
      "between --> between\n",
      "wasn --> wasn\n",
      "any --> any\n",
      "same --> same\n",
      "those --> those\n",
      "hadn't --> hadn't\n",
      "themselves --> themselves\n",
      "were --> were\n",
      "been --> been\n",
      "doing --> doing\n",
      "did --> did\n",
      "shan't --> shan't\n",
      "you'll --> you'll\n",
      "during --> during\n",
      "other --> other\n",
      "shouldn't --> shouldn't\n",
      "after --> after\n",
      "you'd --> you'd\n",
      "hasn't --> hasn't\n",
      "that --> that\n",
      "won't --> won't\n",
      "once --> once\n",
      "ve --> ve\n",
      "above --> above\n",
      "with --> with\n",
      "couldn't --> couldn't\n",
      "will --> will\n",
      "their --> their\n",
      "off --> off\n",
      "it --> it\n",
      "you're --> you're\n",
      "what --> what\n",
      "and --> and\n",
      "until --> until\n",
      "below --> below\n",
      "having --> having\n",
      "this --> this\n",
      "its --> it\n",
      "him --> him\n",
      "most --> most\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for word in stop_words:\n",
    "    print(word+' --> '+lemmatizer.lemmatize(word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
